import time
from langdetect import detect
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ================================
# Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠ
MODEL_PATH = Path(r"/path/to/Qwen2.5-7B-Instruct")  # Ø¹Ø¯Ù‘Ù„ Ø§Ù„Ù…Ø³Ø§Ø± Ø­Ø³Ø¨ Ù…ÙƒØ§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ±ÙØ±

# ================================
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙˆÙƒÙ†ÙŠØ²Ø± ÙˆØ§Ù„Ù…ÙˆØ¯ÙŠÙ„
tokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH), local_files_only=True)
model = AutoModelForCausalLM.from_pretrained(
    str(MODEL_PATH),
    local_files_only=True,
    device_map="auto",
    torch_dtype=torch.float16
)

# ================================
# Ø®Ø±ÙŠØ·Ø© ØªØ­ÙˆÙŠÙ„ ÙƒÙˆØ¯ Ø§Ù„Ù„ØºØ© Ø¥Ù„Ù‰ Ø§Ø³Ù… Ù„ØºØ©
language_map = {
    "ar": "Arabic",
    "en": "English",
    "fr": "French",
    "es": "Spanish",
    "de": "German",
    "it": "Italian",
    "tr": "Turkish",
    "pt": "Portuguese",
    "ru": "Russian"
}

# ================================
def generate_trump_response_local(user_text: str, save_path: str = "documentary_intro.txt", max_tokens: int = 500) -> str:
    """
    ØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ø¨Ø£Ø³Ù„ÙˆØ¨ ØªØ±Ø§Ù…Ø¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Qwen2.5-7B-Instruct Ù…Ø­Ù„ÙŠÙ‹Ø§.
    Ù†ÙØ³ Ø®ØµØ§Ø¦Øµ Ø§Ù„ÙØ§Ù†ÙƒØ´Ù† Ø§Ù„Ø£ØµÙ„ÙŠØ©: ÙƒØ´Ù Ø§Ù„Ù„ØºØ©ØŒ Ø­ÙØ¸ Ø§Ù„Ù†Ø§ØªØ¬ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø±ÙˆÙ…Ø¨Øª Ù…Ø­Ø¯Ø¯.
    """
    if not user_text.strip():
        raise ValueError("âŒ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„ ÙØ§Ø±Øº.")

    # 1ï¸âƒ£ ÙƒØ´Ù Ø§Ù„Ù„ØºØ©
    language = detect(user_text)
    print(f"ğŸ“Œ Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§: {language} âœ… Ù…ØªØ§Ø¨Ø¹Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ©.")
    language_name = language_map.get(language, "English")

    # 2ï¸âƒ£ Ø­ÙØ¸ Ø§Ù„Ù„ØºØ© ÙÙŠ Ù…Ù„Ù
    with open("detected_lang.txt", "w", encoding="utf-8") as f:
        f.write(language)

    # 3ï¸âƒ£ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø¨Ø£Ø³Ù„ÙˆØ¨ ØªØ±Ø§Ù…Ø¨
    prompt = f"""
    You are now speaking as Donald J. Trump. Respond in his distinctive speaking style: confident, bold, sometimes repetitive, rich in catchphrases â€” but also appropriate to the context.

    â›” Do not go off-topic. Do not introduce yourself unless specifically asked.

    âœ… If the user greets you (e.g., "hello", "how are you"), reply naturally and briefly as Trump would in real life â€” warm, short, and direct.

    âœ… If the user asks a question, answer it directly with Trump's tone, but adjust the length based on the complexity of the input:
    - For simple questions, respond briefly.
    - For deeper or complex prompts, expand as needed using Trump's characteristic way of speaking.

    Respond only to the following input as Trump:

    {user_text}

    Always respond in this language: {language_name}.
    """

    # 4ï¸âƒ£ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ù…Ø­Ù„ÙŠÙ‹Ø§ Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø°Ø§ Ø­ØµÙ„ Ø®Ø·Ø£
    retries = 3
    delay = 5
    final_response = None

    for attempt in range(retries):
        try:
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7
            )
            final_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            break
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ: {e}")
            if attempt < retries - 1:
                print(f"â³ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¹Ø¯ {delay} Ø«Ø§Ù†ÙŠØ©...")
                time.sleep(delay)

    if not final_response:
        raise Exception("âŒ ÙØ´Ù„ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ø¨Ø¹Ø¯ Ø¹Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø§Øª.")

    # 5ï¸âƒ£ Ø­ÙØ¸ Ø§Ù„Ù†Øµ ÙÙŠ Ù…Ù„Ù
    with open(save_path, "w", encoding="utf-8") as file:
        file.write(final_response)

    print(f"ğŸ‰ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ ÙˆØ­ÙØ¸Ù‡ ÙÙŠ {save_path}")
    return final_response

# ================================
# Ù…Ø«Ø§Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    prompt = "Hello, who are you?"
    response = generate_trump_response_local(prompt)
    print("Prompt:", prompt)
    print("Response:", response)
